<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Neural Network Architecture Visual Guide</title>
    <style>
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }

      body {
        font-family: "Segoe UI", Tahoma, Geneva, Verdana, sans-serif;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 20px;
        color: #2d3748;
      }

      .container {
        max-width: 1400px;
        margin: 0 auto;
      }

      .slide {
        background: white;
        border-radius: 20px;
        padding: 50px;
        margin-bottom: 30px;
        box-shadow: 0 20px 60px rgba(0, 0, 0, 0.3);
        min-height: 600px;
      }

      .title-slide {
        text-align: center;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
      }

      h1 {
        font-size: 3em;
        color: #667eea;
        margin-bottom: 20px;
      }

      h2 {
        font-size: 2.2em;
        color: #667eea;
        margin-bottom: 30px;
        border-bottom: 3px solid #667eea;
        padding-bottom: 15px;
      }

      h3 {
        font-size: 1.5em;
        color: #764ba2;
        margin: 20px 0 10px 0;
      }

      .subtitle {
        font-size: 1.3em;
        color: #718096;
        margin-bottom: 30px;
      }

      .author-info {
        font-size: 1.1em;
        color: #4a5568;
        margin-top: 20px;
      }

      .component-grid {
        display: grid;
        grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
        gap: 25px;
        margin-top: 30px;
      }

      .component-card {
        background: linear-gradient(135deg, #f7fafc 0%, #edf2f7 100%);
        border-radius: 12px;
        padding: 25px;
        border-left: 5px solid #667eea;
        transition: transform 0.3s ease, box-shadow 0.3s ease;
      }

      .component-card:hover {
        transform: translateY(-5px);
        box-shadow: 0 12px 24px rgba(102, 126, 234, 0.2);
      }

      .component-title {
        font-size: 1.4em;
        font-weight: bold;
        color: #667eea;
        margin-bottom: 15px;
        display: flex;
        align-items: center;
        gap: 10px;
      }

      .component-icon {
        width: 30px;
        height: 30px;
        background: #667eea;
        border-radius: 50%;
        display: flex;
        align-items: center;
        justify-content: center;
        color: white;
        font-weight: bold;
      }

      .component-description {
        color: #4a5568;
        line-height: 1.8;
        font-size: 1em;
      }

      .architecture-diagram {
        margin: 40px 0;
        padding: 30px;
        background: #f7fafc;
        border-radius: 15px;
        text-align: center;
      }

      .neural-network-viz {
        display: flex;
        justify-content: space-around;
        align-items: center;
        margin: 40px 0;
        padding: 40px;
        background: white;
        border-radius: 15px;
        box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1);
      }

      .layer {
        display: flex;
        flex-direction: column;
        align-items: center;
        gap: 15px;
      }

      .layer-label {
        font-weight: bold;
        color: #667eea;
        font-size: 1.1em;
        margin-bottom: 10px;
      }

      .neuron {
        width: 50px;
        height: 50px;
        border-radius: 50%;
        background: linear-gradient(135deg, #667eea, #764ba2);
        display: flex;
        align-items: center;
        justify-content: center;
        color: white;
        font-weight: bold;
        box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
        position: relative;
      }

      .neuron-small {
        width: 35px;
        height: 35px;
        font-size: 0.8em;
      }

      .connection-line {
        width: 80px;
        height: 2px;
        background: linear-gradient(90deg, #667eea, #764ba2);
        opacity: 0.3;
      }

      .flow-arrow {
        font-size: 2em;
        color: #667eea;
        font-weight: bold;
      }

      .key-points {
        background: #f7fafc;
        border-radius: 12px;
        padding: 25px;
        margin: 20px 0;
      }

      .key-points ul {
        list-style-position: inside;
        color: #4a5568;
        line-height: 2;
      }

      .key-points li {
        margin: 10px 0;
        padding-left: 10px;
      }

      .highlight-box {
        background: linear-gradient(135deg, #fef5e7, #fdebd0);
        border-left: 5px solid #f39c12;
        padding: 20px;
        border-radius: 8px;
        margin: 20px 0;
      }

      .comparison-table {
        width: 100%;
        border-collapse: collapse;
        margin: 20px 0;
      }

      .comparison-table th {
        background: #667eea;
        color: white;
        padding: 15px;
        text-align: left;
      }

      .comparison-table td {
        padding: 15px;
        border-bottom: 1px solid #e2e8f0;
      }

      .comparison-table tr:hover {
        background: #f7fafc;
      }

      .summary-grid {
        display: grid;
        grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
        gap: 20px;
        margin-top: 30px;
      }

      .insight-card {
        background: linear-gradient(135deg, #ebf8ff, #bee3f8);
        border-radius: 12px;
        padding: 25px;
        border-top: 4px solid #3182ce;
      }

      .insight-card h4 {
        color: #2c5282;
        margin-bottom: 10px;
        font-size: 1.2em;
      }

      .insight-card p {
        color: #2d3748;
        line-height: 1.6;
      }

      footer {
        text-align: center;
        color: white;
        padding: 20px;
        font-size: 0.9em;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <!-- Slide 1: Title -->
      <div class="slide title-slide">
        <h1>Understanding Neural Network Architecture</h1>
        <p class="subtitle">A Visual Guide to Core Components and Data Flow</p>
        <p class="author-info">
          <strong>Muqeeth Mohammad</strong><br />
          AIML-501 Model Development<br />
          Indiana Wesleyan University<br />
        </p>
      </div>

      <!-- Slide 2: Neural Network Overview -->
      <div class="slide">
        <h2>What is a Neural Network?</h2>
        <p style="font-size: 1.1em; line-height: 1.8; margin-bottom: 30px">
          Neural networks are computational models inspired by the human brain's
          structure. They consist of interconnected nodes (neurons) organized in
          layers that process information by passing signals forward and
          adjusting internal parameters through learning. These networks excel
          at finding complex patterns in data, making them the foundation of
          modern artificial intelligence applications.
        </p>

        <div class="highlight-box">
          <h3 style="color: #d68910; margin-bottom: 15px">
            Key Characteristics
          </h3>
          <p style="line-height: 1.8">
            Neural networks learn from examples rather than following explicit
            programmed rules. They adjust millions of internal parameters
            (weights) through a process called training, where they gradually
            improve their performance by minimizing prediction errors. This
            ability to learn complex, non-linear relationships makes them
            powerful tools for tasks like image recognition, language
            translation, and predictive analytics.
          </p>
        </div>
      </div>

      <!-- Slide 3: Neural Network Architecture Diagram -->
      <div class="slide">
        <h2>Neural Network Architecture: Data Flow</h2>
        <p style="font-size: 1.1em; margin-bottom: 30px">
          Below is a visual representation of how data flows through a neural
          network, from input to output:
        </p>

        <div class="neural-network-viz">
          <div class="layer">
            <div class="layer-label">Input Layer</div>
            <div class="neuron">X₁</div>
            <div class="neuron">X₂</div>
            <div class="neuron">X₃</div>
          </div>

          <div class="flow-arrow">→</div>

          <div class="layer">
            <div class="layer-label">Hidden Layer 1</div>
            <div class="neuron neuron-small">H₁</div>
            <div class="neuron neuron-small">H₂</div>
            <div class="neuron neuron-small">H₃</div>
            <div class="neuron neuron-small">H₄</div>
          </div>

          <div class="flow-arrow">→</div>

          <div class="layer">
            <div class="layer-label">Hidden Layer 2</div>
            <div class="neuron neuron-small">H₅</div>
            <div class="neuron neuron-small">H₆</div>
            <div class="neuron neuron-small">H₇</div>
            <div class="neuron neuron-small">H₈</div>
          </div>

          <div class="flow-arrow">→</div>

          <div class="layer">
            <div class="layer-label">Output Layer</div>
            <div class="neuron">Y₁</div>
            <div class="neuron">Y₂</div>
          </div>
        </div>

        <div class="key-points">
          <h3>Data Flow Process:</h3>
          <ul>
            <li>
              <strong>Step 1:</strong> Input features enter through the input
              layer
            </li>
            <li>
              <strong>Step 2:</strong> Each hidden layer transforms the data
              using weights and activation functions
            </li>
            <li>
              <strong>Step 3:</strong> Information is progressively refined as
              it moves through layers
            </li>
            <li>
              <strong>Step 4:</strong> The output layer produces final
              predictions or classifications
            </li>
            <li>
              <strong>Step 5:</strong> During training, errors flow backward to
              adjust weights (backpropagation)
            </li>
          </ul>
        </div>
      </div>

      <!-- Slide 4: Component 1 - Layers -->
      <div class="slide">
        <h2>Component 1: Layers</h2>

        <div class="component-card">
          <div class="component-title">
            <div class="component-icon">L</div>
            What are Layers?
          </div>
          <div class="component-description">
            Layers are the fundamental organizational units in a neural network.
            They group neurons that perform similar transformations on data. A
            network typically contains three types of layers: an input layer
            that receives raw data, one or more hidden layers that perform
            intermediate computations, and an output layer that produces final
            predictions.
          </div>
        </div>

        <h3 style="margin-top: 30px">Types of Layers</h3>

        <table class="comparison-table">
          <tr>
            <th>Layer Type</th>
            <th>Purpose</th>
            <th>Example</th>
          </tr>
          <tr>
            <td><strong>Input Layer</strong></td>
            <td>
              Receives raw features from the dataset. Each neuron represents one
              feature.
            </td>
            <td>
              For house price prediction: square footage, number of bedrooms,
              location coordinates
            </td>
          </tr>
          <tr>
            <td><strong>Hidden Layers</strong></td>
            <td>
              Perform intermediate transformations and feature extraction. More
              layers enable learning of more complex patterns.
            </td>
            <td>
              In image recognition, early layers detect edges, middle layers
              identify shapes, deeper layers recognize objects
            </td>
          </tr>
          <tr>
            <td><strong>Output Layer</strong></td>
            <td>
              Produces final predictions. Size depends on the task: 1 neuron for
              regression, multiple for classification.
            </td>
            <td>
              For binary classification (spam/not spam): 1 neuron with
              probability. For multi-class (10 digits): 10 neurons
            </td>
          </tr>
        </table>

        <div class="highlight-box">
          <h4>Key Insight from Neural Network Playground:</h4>
          <p>
            Adding more hidden layers increases the network's capacity to learn
            complex patterns. However, too many layers can lead to overfitting
            (memorizing training data) and longer training times. The optimal
            depth depends on data complexity: simple patterns may need only 1-2
            hidden layers, while complex tasks like image recognition benefit
            from dozens of layers.
          </p>
        </div>
      </div>

      <!-- Slide 5: Component 2 - Neurons -->
      <div class="slide">
        <h2>Component 2: Neurons</h2>

        <div class="component-card">
          <div class="component-title">
            <div class="component-icon">N</div>
            What are Neurons?
          </div>
          <div class="component-description">
            Neurons (also called nodes or units) are the basic computational
            units in a neural network. Each neuron receives multiple inputs,
            applies a mathematical transformation, and produces a single output.
            This process mimics how biological neurons in the brain receive
            signals from dendrites, process them, and transmit results through
            axons.
          </div>
        </div>

        <h3 style="margin-top: 30px">How Neurons Work</h3>

        <div class="key-points">
          <p style="margin-bottom: 15px; font-size: 1.1em">
            <strong>The Neuron Computation Process:</strong>
          </p>
          <ul>
            <li>
              <strong>Step 1 - Weighted Sum:</strong> Each input is multiplied
              by its corresponding weight, then all products are summed: z =
              (w₁×x₁) + (w₂×x₂) + ... + (wₙ×xₙ) + bias
            </li>
            <li>
              <strong>Step 2 - Add Bias:</strong> A bias term is added to shift
              the activation threshold, giving the network more flexibility
            </li>
            <li>
              <strong>Step 3 - Activation Function:</strong> The weighted sum
              passes through an activation function to introduce non-linearity:
              output = activation(z)
            </li>
            <li>
              <strong>Step 4 - Forward Propagation:</strong> The neuron's output
              becomes input for neurons in the next layer
            </li>
          </ul>
        </div>

        <div class="highlight-box">
          <h4>Practical Example:</h4>
          <p>
            Imagine a neuron deciding if someone should receive a loan. It
            receives inputs like credit score (x₁=750), income (x₂=75000), and
            debt (x₃=15000). Each input has a learned weight reflecting its
            importance: w₁=0.8 (credit score matters most), w₂=0.5, w₃=-0.7
            (debt is negative). The neuron calculates: z = (0.8×750) +
            (0.5×75000) + (-0.7×15000) + bias = 27,600. This passes through an
            activation function producing a probability between 0 and 1, where
            values above 0.5 might indicate loan approval.
          </p>
        </div>

        <h3 style="margin-top: 30px">Number of Neurons Impact</h3>
        <p style="line-height: 1.8">
          The number of neurons in each layer affects the network's learning
          capacity. More neurons per layer increase the network's ability to
          capture subtle patterns but also increase computational cost and risk
          of overfitting. From my experiments in the Neural Network Playground,
          I observed that adding neurons to hidden layers improved performance
          on complex datasets but showed diminishing returns beyond a certain
          point.
        </p>
      </div>

      <!-- Slide 6: Component 3 - Weights -->
      <div class="slide">
        <h2>Component 3: Weights</h2>

        <div class="component-card">
          <div class="component-title">
            <div class="component-icon">W</div>
            What are Weights?
          </div>
          <div class="component-description">
            Weights are numerical parameters that determine the strength and
            direction of connections between neurons. They represent the
            knowledge learned by the network during training. Each connection
            between neurons has its own weight, which can be positive (exciting
            the next neuron), negative (inhibiting it), or near zero (minimal
            influence). The process of learning in neural networks is
            essentially the process of adjusting these weights to minimize
            prediction errors.
          </div>
        </div>

        <h3 style="margin-top: 30px">Role of Weights in Learning</h3>

        <div class="component-grid">
          <div class="component-card">
            <h4 style="color: #667eea">Initial Weights</h4>
            <p>
              Weights start with random small values (usually between -0.5 and
              0.5). This randomness is crucial because if all weights started
              identical, all neurons would learn the same patterns, defeating
              the purpose of having multiple neurons.
            </p>
          </div>

          <div class="component-card">
            <h4 style="color: #667eea">Weight Updates</h4>
            <p>
              During training, weights are continuously adjusted based on
              prediction errors. If a prediction is too high, weights that
              contributed to that prediction are decreased. If too low, they're
              increased. This adjustment happens millions of times.
            </p>
          </div>

          <div class="component-card">
            <h4 style="color: #667eea">Weight Magnitude</h4>
            <p>
              The absolute value of a weight indicates feature importance. A
              weight of 2.5 has stronger influence than 0.3. During training,
              important features naturally develop larger weights while
              irrelevant features remain near zero.
            </p>
          </div>

          <div class="component-card">
            <h4 style="color: #667eea">Weight Sign</h4>
            <p>
              Positive weights amplify signals (excitatory connections), while
              negative weights suppress them (inhibitory connections). This
              allows networks to learn both what features to look for and what
              to ignore.
            </p>
          </div>
        </div>

        <div class="highlight-box" style="margin-top: 30px">
          <h4>Observation from Neural Network Playground:</h4>
          <p>
            While experimenting with different datasets, I noticed that the
            weight visualization showed thicker, darker lines for important
            connections and thinner, lighter lines for less important ones. On
            the spiral dataset with high noise, the network developed very
            different weight patterns compared to the simpler circular dataset.
            This visual representation helped me understand that learning is
            really about finding the right weight configuration that maps inputs
            to correct outputs.
          </p>
        </div>
      </div>

      <!-- Slide 7: Component 4 - Activation Functions -->
      <div class="slide">
        <h2>Component 4: Activation Functions</h2>

        <div class="component-card">
          <div class="component-title">
            <div class="component-icon">σ</div>
            What are Activation Functions?
          </div>
          <div class="component-description">
            Activation functions introduce non-linearity into neural networks,
            enabling them to learn complex patterns. Without activation
            functions, a neural network would simply be a series of linear
            transformations, which could be collapsed into a single linear
            equation regardless of depth. Activation functions allow networks to
            model curved decision boundaries and capture intricate relationships
            in data.
          </div>
        </div>

        <h3 style="margin-top: 30px">Common Activation Functions</h3>

        <table class="comparison-table">
          <tr>
            <th>Function</th>
            <th>Formula</th>
            <th>Range</th>
            <th>Use Case</th>
          </tr>
          <tr>
            <td><strong>ReLU</strong><br />(Rectified Linear Unit)</td>
            <td>f(x) = max(0, x)</td>
            <td>[0, ∞)</td>
            <td>
              Most popular for hidden layers. Fast to compute and helps prevent
              vanishing gradients. Sets negative values to zero.
            </td>
          </tr>
          <tr>
            <td><strong>Sigmoid</strong></td>
            <td>f(x) = 1 / (1 + e⁻ˣ)</td>
            <td>(0, 1)</td>
            <td>
              Output layers for binary classification. Produces probabilities
              but can suffer from vanishing gradients in deep networks.
            </td>
          </tr>
          <tr>
            <td><strong>Tanh</strong><br />(Hyperbolic Tangent)</td>
            <td>f(x) = (eˣ - e⁻ˣ) / (eˣ + e⁻ˣ)</td>
            <td>(-1, 1)</td>
            <td>
              Hidden layers when centered data is beneficial. Similar to sigmoid
              but zero-centered, which can speed up learning.
            </td>
          </tr>
          <tr>
            <td><strong>Softmax</strong></td>
            <td>f(xᵢ) = eˣⁱ / Σ(eˣʲ)</td>
            <td>(0, 1), sum=1</td>
            <td>
              Output layer for multi-class classification. Converts raw scores
              into probability distribution across classes.
            </td>
          </tr>
          <tr>
            <td><strong>Linear</strong></td>
            <td>f(x) = x</td>
            <td>(-∞, ∞)</td>
            <td>
              Output layer for regression tasks predicting continuous values
              (e.g., house prices, temperature).
            </td>
          </tr>
        </table>

        <div class="highlight-box" style="margin-top: 30px">
          <h4>Insights from Neural Network Playground Experiments:</h4>
          <p>
            I tested different activation functions on the same dataset and
            observed significant differences in learning speed and final
            accuracy. ReLU consistently trained faster and achieved better
            results on complex patterns like spirals. Tanh performed well on
            simpler datasets but struggled with very deep networks. Sigmoid
            worked best for binary classification but showed slow convergence.
            The Linear activation failed completely on non-linear patterns like
            XOR, demonstrating why non-linearity is essential. This hands-on
            experimentation reinforced that activation function choice
            significantly impacts model performance.
          </p>
        </div>
      </div>

      <!-- Slide 8: Component 5 - Loss Functions -->
      <div class="slide">
        <h2>Component 5: Loss Functions</h2>

        <div class="component-card">
          <div class="component-title">
            <div class="component-icon">L</div>
            What are Loss Functions?
          </div>
          <div class="component-description">
            Loss functions (also called cost functions or objective functions)
            measure how wrong a neural network's predictions are compared to
            actual values. They provide a single numerical score representing
            model performance. During training, the network's goal is to
            minimize this loss by adjusting weights. The choice of loss function
            depends on the type of problem being solved.
          </div>
        </div>

        <h3 style="margin-top: 30px">Common Loss Functions</h3>

        <div class="component-grid">
          <div class="component-card">
            <h4 style="color: #667eea">Mean Squared Error (MSE)</h4>
            <p style="margin: 10px 0">
              <strong>Formula:</strong> MSE = (1/n) × Σ(yᵢ - ŷᵢ)²
            </p>
            <p>
              <strong>Use:</strong> Regression problems (predicting continuous
              values)<br />
              <strong>How it works:</strong> Calculates the average squared
              difference between predicted and actual values. Larger errors are
              penalized more heavily due to squaring. This encourages the
              network to avoid large mistakes.
            </p>
          </div>

          <div class="component-card">
            <h4 style="color: #667eea">Binary Cross-Entropy</h4>
            <p style="margin: 10px 0">
              <strong>Formula:</strong> BCE = -[y×log(ŷ) + (1-y)×log(1-ŷ)]
            </p>
            <p>
              <strong>Use:</strong> Binary classification (two classes)<br />
              <strong>How it works:</strong> Measures how far predicted
              probabilities are from actual binary labels (0 or 1). Penalizes
              confident wrong predictions more heavily than uncertain ones.
            </p>
          </div>

          <div class="component-card">
            <h4 style="color: #667eea">Categorical Cross-Entropy</h4>
            <p style="margin: 10px 0">
              <strong>Formula:</strong> CCE = -Σ(yᵢ × log(ŷᵢ))
            </p>
            <p>
              <strong>Use:</strong> Multi-class classification (more than two
              classes)<br />
              <strong>How it works:</strong> Extends binary cross-entropy to
              multiple classes. Compares predicted probability distribution
              against true class distribution.
            </p>
          </div>
        </div>

        <h3 style="margin-top: 30px">Why Loss Functions Matter</h3>
        <div class="key-points">
          <ul>
            <li>
              <strong>Training Guidance:</strong> Loss functions provide the
              signal for how to adjust weights. Without them, the network has no
              way to know if it's improving.
            </li>
            <li>
              <strong>Optimization Target:</strong> The entire training process
              aims to find weights that minimize the loss function value.
            </li>
            <li>
              <strong>Performance Monitoring:</strong> Tracking loss over
              training iterations shows whether the model is learning.
              Decreasing loss indicates improvement.
            </li>
            <li>
              <strong>Problem-Specific:</strong> Different tasks require
              different loss functions. Using MSE for classification or
              cross-entropy for regression would produce poor results.
            </li>
          </ul>
        </div>

        <div class="highlight-box">
          <h4>Neural Network Playground Observation:</h4>
          <p>
            The playground displays training and test loss in real-time. I
            observed that on datasets with noise, the training loss continued
            decreasing while test loss eventually plateaued or increased,
            indicating overfitting. This visual feedback helped me understand
            that minimizing training loss alone isn't the goal—we need to
            balance it with generalization to unseen data.
          </p>
        </div>
      </div>

      <!-- Slide 9: Component 6 - Optimization Algorithms -->
      <div class="slide">
        <h2>Component 6: Optimization Algorithms</h2>

        <div class="component-card">
          <div class="component-title">
            <div class="component-icon">⚙</div>
            What are Optimization Algorithms?
          </div>
          <div class="component-description">
            Optimization algorithms (also called optimizers) are the methods
            used to adjust neural network weights during training to minimize
            the loss function. They determine how the network learns from its
            mistakes. The optimizer takes the gradient (direction and magnitude
            of steepest loss increase) and uses it to update weights in the
            opposite direction, gradually moving toward better predictions.
          </div>
        </div>

        <h3 style="margin-top: 30px">Popular Optimization Algorithms</h3>

        <table class="comparison-table">
          <tr>
            <th>Algorithm</th>
            <th>How It Works</th>
            <th>Advantages</th>
            <th>Disadvantages</th>
          </tr>
          <tr>
            <td><strong>SGD</strong><br />(Stochastic Gradient Descent)</td>
            <td>
              Updates weights using gradient calculated from each training
              example or small batch. Simple and foundational approach.
            </td>
            <td>
              Simple, works well with proper tuning, low memory requirements
            </td>
            <td>
              Slow convergence, sensitive to learning rate, can get stuck in
              local minima
            </td>
          </tr>
          <tr>
            <td><strong>Adam</strong><br />(Adaptive Moment Estimation)</td>
            <td>
              Combines momentum and adaptive learning rates. Maintains moving
              averages of gradients and their squares for each parameter.
            </td>
            <td>
              Fast convergence, works well with default parameters, handles
              sparse gradients well
            </td>
            <td>Can overfit on some problems, requires more memory</td>
          </tr>
          <tr>
            <td><strong>RMSprop</strong></td>
            <td>
              Adapts learning rate for each parameter based on recent gradient
              magnitudes. Good for handling different scales of features.
            </td>
            <td>
              Works well on non-stationary problems, good for recurrent neural
              networks
            </td>
            <td>
              Still requires learning rate tuning, can be slow on some problems
            </td>
          </tr>
          <tr>
            <td><strong>Momentum</strong></td>
            <td>
              Adds a fraction of previous update to current update, helping
              accelerate in relevant directions
            </td>
          </tr>
        </table>
      </div>
    </div>
  </body>
</html>
