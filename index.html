<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Neural Network Architecture Visual Guide</title>
    <style>
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }

      body {
        font-family: "Segoe UI", Tahoma, Geneva, Verdana, sans-serif;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 20px;
        color: #2d3748;
      }

      .container {
        max-width: 1400px;
        margin: 0 auto;
      }

      .slide {
        background: white;
        border-radius: 20px;
        padding: 50px;
        margin-bottom: 30px;
        box-shadow: 0 20px 60px rgba(0, 0, 0, 0.3);
        min-height: 600px;
      }

      .title-slide {
        text-align: center;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
      }

      h1 {
        font-size: 3em;
        color: #667eea;
        margin-bottom: 20px;
      }

      h2 {
        font-size: 2.2em;
        color: #667eea;
        margin-bottom: 30px;
        border-bottom: 3px solid #667eea;
        padding-bottom: 15px;
      }

      h3 {
        font-size: 1.5em;
        color: #764ba2;
        margin: 20px 0 10px 0;
      }

      .subtitle {
        font-size: 1.3em;
        color: #718096;
        margin-bottom: 30px;
      }

      .author-info {
        font-size: 1.1em;
        color: #4a5568;
        margin-top: 20px;
      }

      .component-grid {
        display: grid;
        grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
        gap: 25px;
        margin-top: 30px;
      }

      .component-card {
        background: linear-gradient(135deg, #f7fafc 0%, #edf2f7 100%);
        border-radius: 12px;
        padding: 25px;
        border-left: 5px solid #667eea;
        transition: transform 0.3s ease, box-shadow 0.3s ease;
      }

      .component-card:hover {
        transform: translateY(-5px);
        box-shadow: 0 12px 24px rgba(102, 126, 234, 0.2);
      }

      .component-title {
        font-size: 1.4em;
        font-weight: bold;
        color: #667eea;
        margin-bottom: 15px;
        display: flex;
        align-items: center;
        gap: 10px;
      }

      .component-icon {
        width: 30px;
        height: 30px;
        background: #667eea;
        border-radius: 50%;
        display: flex;
        align-items: center;
        justify-content: center;
        color: white;
        font-weight: bold;
      }

      .component-description {
        color: #4a5568;
        line-height: 1.8;
        font-size: 1em;
      }

      .architecture-diagram {
        margin: 40px 0;
        padding: 30px;
        background: #f7fafc;
        border-radius: 15px;
        text-align: center;
      }

      .neural-network-viz {
        display: flex;
        justify-content: space-around;
        align-items: center;
        margin: 40px 0;
        padding: 40px;
        background: white;
        border-radius: 15px;
        box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1);
      }

      .layer {
        display: flex;
        flex-direction: column;
        align-items: center;
        gap: 15px;
      }

      .layer-label {
        font-weight: bold;
        color: #667eea;
        font-size: 1.1em;
        margin-bottom: 10px;
      }

      .neuron {
        width: 50px;
        height: 50px;
        border-radius: 50%;
        background: linear-gradient(135deg, #667eea, #764ba2);
        display: flex;
        align-items: center;
        justify-content: center;
        color: white;
        font-weight: bold;
        box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
        position: relative;
      }

      .neuron-small {
        width: 35px;
        height: 35px;
        font-size: 0.8em;
      }

      .connection-line {
        width: 80px;
        height: 2px;
        background: linear-gradient(90deg, #667eea, #764ba2);
        opacity: 0.3;
      }

      .flow-arrow {
        font-size: 2em;
        color: #667eea;
        font-weight: bold;
      }

      .key-points {
        background: #f7fafc;
        border-radius: 12px;
        padding: 25px;
        margin: 20px 0;
      }

      .key-points ul {
        list-style-position: inside;
        color: #4a5568;
        line-height: 2;
      }

      .key-points li {
        margin: 10px 0;
        padding-left: 10px;
      }

      .highlight-box {
        background: linear-gradient(135deg, #fef5e7, #fdebd0);
        border-left: 5px solid #f39c12;
        padding: 20px;
        border-radius: 8px;
        margin: 20px 0;
      }

      .comparison-table {
        width: 100%;
        border-collapse: collapse;
        margin: 20px 0;
      }

      .comparison-table th {
        background: #667eea;
        color: white;
        padding: 15px;
        text-align: left;
      }

      .comparison-table td {
        padding: 15px;
        border-bottom: 1px solid #e2e8f0;
      }

      .comparison-table tr:hover {
        background: #f7fafc;
      }

      .summary-grid {
        display: grid;
        grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
        gap: 20px;
        margin-top: 30px;
      }

      .insight-card {
        background: linear-gradient(135deg, #ebf8ff, #bee3f8);
        border-radius: 12px;
        padding: 25px;
        border-top: 4px solid #3182ce;
      }

      .insight-card h4 {
        color: #2c5282;
        margin-bottom: 10px;
        font-size: 1.2em;
      }

      .insight-card p {
        color: #2d3748;
        line-height: 1.6;
      }

      footer {
        text-align: center;
        color: white;
        padding: 20px;
        font-size: 0.9em;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <!-- Slide 1: Title -->
      <div class="slide title-slide">
        <h1>Understanding Neural Network Architecture</h1>
        <p class="subtitle">A Visual Guide to Core Components and Data Flow</p>
        <p class="author-info">
          <strong>Muqeeth Mohammad</strong><br />
          AIML-501 Model Development<br />
          Indiana Wesleyan University<br />
        </p>
      </div>

      <!-- Slide 2: Neural Network Overview -->
      <div class="slide">
        <h2>What is a Neural Network?</h2>
        <p style="font-size: 1.1em; line-height: 1.8; margin-bottom: 30px">
          Neural networks are computational models inspired by the human brain's
          structure. They consist of interconnected nodes (neurons) organized in
          layers that process information by passing signals forward and
          adjusting internal parameters through learning. These networks excel
          at finding complex patterns in data, making them the foundation of
          modern artificial intelligence applications.
        </p>

        <div class="highlight-box">
          <h3 style="color: #d68910; margin-bottom: 15px">
            Key Characteristics
          </h3>
          <p style="line-height: 1.8">
            Neural networks learn from examples rather than following explicit
            programmed rules. They adjust millions of internal parameters
            (weights) through a process called training, where they gradually
            improve their performance by minimizing prediction errors. This
            ability to learn complex, non-linear relationships makes them
            powerful tools for tasks like image recognition, language
            translation, and predictive analytics.
          </p>
        </div>
      </div>

      <!-- Slide 3: Neural Network Architecture Diagram -->
      <div class="slide">
        <h2>Neural Network Architecture: Data Flow</h2>
        <p style="font-size: 1.1em; margin-bottom: 30px">
          Below is a visual representation of how data flows through a neural
          network, from input to output:
        </p>

        <div class="neural-network-viz">
          <div class="layer">
            <div class="layer-label">Input Layer</div>
            <div class="neuron">X₁</div>
            <div class="neuron">X₂</div>
            <div class="neuron">X₃</div>
          </div>

          <div class="flow-arrow">→</div>

          <div class="layer">
            <div class="layer-label">Hidden Layer 1</div>
            <div class="neuron neuron-small">H₁</div>
            <div class="neuron neuron-small">H₂</div>
            <div class="neuron neuron-small">H₃</div>
            <div class="neuron neuron-small">H₄</div>
          </div>

          <div class="flow-arrow">→</div>

          <div class="layer">
            <div class="layer-label">Hidden Layer 2</div>
            <div class="neuron neuron-small">H₅</div>
            <div class="neuron neuron-small">H₆</div>
            <div class="neuron neuron-small">H₇</div>
            <div class="neuron neuron-small">H₈</div>
          </div>

          <div class="flow-arrow">→</div>

          <div class="layer">
            <div class="layer-label">Output Layer</div>
            <div class="neuron">Y₁</div>
            <div class="neuron">Y₂</div>
          </div>
        </div>

        <div class="key-points">
          <h3>Data Flow Process:</h3>
          <ul>
            <li>
              <strong>Step 1:</strong> Input features enter through the input
              layer
            </li>
            <li>
              <strong>Step 2:</strong> Each hidden layer transforms the data
              using weights and activation functions
            </li>
            <li>
              <strong>Step 3:</strong> Information is progressively refined as
              it moves through layers
            </li>
            <li>
              <strong>Step 4:</strong> The output layer produces final
              predictions or classifications
            </li>
            <li>
              <strong>Step 5:</strong> During training, errors flow backward to
              adjust weights (backpropagation)
            </li>
          </ul>
        </div>
      </div>

      <!-- Slide 4: Component 1 - Layers -->
      <div class="slide">
        <h2>Component 1: Layers</h2>

        <div class="component-card">
          <div class="component-title">
            <div class="component-icon">L</div>
            What are Layers?
          </div>
          <div class="component-description">
            Layers are the fundamental organizational units in a neural network.
            They group neurons that perform similar transformations on data. A
            network typically contains three types of layers: an input layer
            that receives raw data, one or more hidden layers that perform
            intermediate computations, and an output layer that produces final
            predictions.
          </div>
        </div>

        <h3 style="margin-top: 30px">Types of Layers</h3>

        <table class="comparison-table">
          <tr>
            <th>Layer Type</th>
            <th>Purpose</th>
            <th>Example</th>
          </tr>
          <tr>
            <td><strong>Input Layer</strong></td>
            <td>
              Receives raw features from the dataset. Each neuron represents one
              feature.
            </td>
            <td>
              For house price prediction: square footage, number of bedrooms,
              location coordinates
            </td>
          </tr>
          <tr>
            <td><strong>Hidden Layers</strong></td>
            <td>
              Perform intermediate transformations and feature extraction. More
              layers enable learning of more complex patterns.
            </td>
            <td>
              In image recognition, early layers detect edges, middle layers
              identify shapes, deeper layers recognize objects
            </td>
          </tr>
          <tr>
            <td><strong>Output Layer</strong></td>
            <td>
              Produces final predictions. Size depends on the task: 1 neuron for
              regression, multiple for classification.
            </td>
            <td>
              For binary classification (spam/not spam): 1 neuron with
              probability. For multi-class (10 digits): 10 neurons
            </td>
          </tr>
        </table>

        <div class="highlight-box">
          <h4>Key Insight from Neural Network Playground:</h4>
          <p>
            Adding more hidden layers increases the network's capacity to learn
            complex patterns. However, too many layers can lead to overfitting
            (memorizing training data) and longer training times. The optimal
            depth depends on data complexity: simple patterns may need only 1-2
            hidden layers, while complex tasks like image recognition benefit
            from dozens of layers.
          </p>
        </div>
      </div>

      <!-- Slide 5: Component 2 - Neurons -->
      <div class="slide">
        <h2>Component 2: Neurons</h2>

        <div class="component-card">
          <div class="component-title">
            <div class="component-icon">N</div>
            What are Neurons?
          </div>
          <div class="component-description">
            Neurons (also called nodes or units) are the basic computational
            units in a neural network. Each neuron receives multiple inputs,
            applies a mathematical transformation, and produces a single output.
            This process mimics how biological neurons in the brain receive
            signals from dendrites, process them, and transmit results through
            axons.
          </div>
        </div>

        <h3 style="margin-top: 30px">How Neurons Work</h3>

        <div class="key-points">
          <p style="margin-bottom: 15px; font-size: 1.1em">
            <strong>The Neuron Computation Process:</strong>
          </p>
          <ul>
            <li>
              <strong>Step 1 - Weighted Sum:</strong> Each input is multiplied
              by its corresponding weight, then all products are summed: z =
              (w₁×x₁) + (w₂×x₂) + ... + (wₙ×xₙ) + bias
            </li>
            <li>
              <strong>Step 2 - Add Bias:</strong> A bias term is added to shift
              the activation threshold, giving the network more flexibility
            </li>
            <li>
              <strong>Step 3 - Activation Function:</strong> The weighted sum
              passes through an activation function to introduce non-linearity:
              output = activation(z)
            </li>
            <li>
              <strong>Step 4 - Forward Propagation:</strong> The neuron's output
              becomes input for neurons in the next layer
            </li>
          </ul>
        </div>

        <div class="highlight-box">
          <h4>Practical Example:</h4>
          <p>
            Imagine a neuron deciding if someone should receive a loan. It
            receives inputs like credit score (x₁=750), income (x₂=75000), and
            debt (x₃=15000). Each input has a learned weight reflecting its
            importance: w₁=0.8 (credit score matters most), w₂=0.5, w₃=-0.7
            (debt is negative). The neuron calculates: z = (0.8×750) +
            (0.5×75000) + (-0.7×15000) + bias = 27,600. This passes through an
            activation function producing a probability between 0 and 1, where
            values above 0.5 might indicate loan approval.
          </p>
        </div>

        <h3 style="margin-top: 30px">Number of Neurons Impact</h3>
        <p style="line-height: 1.8">
          The number of neurons in each layer affects the network's learning
          capacity. More neurons per layer increase the network's ability to
          capture subtle patterns but also increase computational cost and risk
          of overfitting. From my experiments in the Neural Network Playground,
          I observed that adding neurons to hidden layers improved performance
          on complex datasets but showed diminishing returns beyond a certain
          point.
        </p>
      </div>

      <!-- Slide 6: Component 3 - Weights -->
      <div class="slide">
        <h2>Component 3: Weights</h2>

        <div class="component-card">
          <div class="component-title">
            <div class="component-icon">W</div>
            What are Weights?
          </div>
          <div class="component-description">
            Weights are numerical parameters that determine the strength and
            direction of connections between neurons. They represent the
            knowledge learned by the network during training. Each connection
            between neurons has its own weight, which can be positive (exciting
            the next neuron), negative (inhibiting it), or near zero (minimal
            influence). The process of learning in neural networks is
            essentially the process of adjusting these weights to minimize
            prediction errors.
          </div>
        </div>

        <h3 style="margin-top: 30px">Role of Weights in Learning</h3>

        <div class="component-grid">
          <div class="component-card">
            <h4 style="color: #667eea">Initial Weights</h4>
            <p>
              Weights start with random small values (usually between -0.5 and
              0.5). This randomness is crucial because if all weights started
              identical, all neurons would learn the same patterns, defeating
              the purpose of having multiple neurons.
            </p>
          </div>

          <div class="component-card">
            <h4 style="color: #667eea">Weight Updates</h4>
            <p>
              During training, weights are continuously adjusted based on
              prediction errors. If a prediction is too high, weights that
              contributed to that prediction are decreased. If too low, they're
              increased. This adjustment happens millions of times.
            </p>
          </div>

          <div class="component-card">
            <h4 style="color: #667eea">Weight Magnitude</h4>
            <p>
              The absolute value of a weight indicates feature importance. A
              weight of 2.5 has stronger influence than 0.3. During training,
              important features naturally develop larger weights while
              irrelevant features remain near zero.
            </p>
          </div>

          <div class="component-card">
            <h4 style="color: #667eea">Weight Sign</h4>
            <p>
              Positive weights amplify signals (excitatory connections), while
              negative weights suppress them (inhibitory connections). This
              allows networks to learn both what features to look for and what
              to ignore.
            </p>
          </div>
        </div>

        <div class="highlight-box" style="margin-top: 30px">
          <h4>Observation from Neural Network Playground:</h4>
          <p>
            While experimenting with different datasets, I noticed that the
            weight visualization showed thicker, darker lines for important
            connections and thinner, lighter lines for less important ones. On
            the spiral dataset with high noise, the network developed very
            different weight patterns compared to the simpler circular dataset.
            This visual representation helped me understand that learning is
            really about finding the right weight configuration that maps inputs
            to correct outputs.
          </p>
        </div>
      </div>

      <!-- Slide 7: Component 4 - Activation Functions -->
      <div class="slide">
        <h2>Component 4: Activation Functions</h2>

        <div class="component-card">
          <div class="component-title">
            <div class="component-icon">σ</div>
            What are Activation Functions?
          </div>
          <div class="component-description">
            Activation functions introduce non-linearity into neural networks,
            enabling them to learn complex patterns. Without activation
            functions, a neural network would simply be a series of linear
            transformations, which could be collapsed into a single linear
            equation regardless of depth. Activation functions allow networks to
            model curved decision boundaries and capture intricate relationships
            in data.
          </div>
        </div>

        <h3 style="margin-top: 30px">Common Activation Functions</h3>

        <table class="comparison-table">
          <tr>
            <th>Function</th>
            <th>Formula</th>
            <th>Range</th>
            <th>Use Case</th>
          </tr>
          <tr>
            <td><strong>ReLU</strong><br />(Rectified Linear Unit)</td>
            <td>f(x) = max(0, x)</td>
            <td>[0, ∞)</td>
            <td>
              Most popular for hidden layers. Fast to compute and helps prevent
              vanishing gradients. Sets negative values to zero.
            </td>
          </tr>
          <tr>
            <td><strong>Sigmoid</strong></td>
            <td>f(x) = 1 / (1 + e⁻ˣ)</td>
            <td>(0, 1)</td>
            <td>
              Output layers for binary classification. Produces probabilities
              but can suffer from vanishing gradients in deep networks.
            </td>
          </tr>
          <tr>
            <td><strong>Tanh</strong><br />(Hyperbolic Tangent)</td>
            <td>f(x) = (eˣ - e⁻ˣ) / (eˣ + e⁻ˣ)</td>
            <td>(-1, 1)</td>
            <td>
              Hidden layers when centered data is beneficial. Similar to sigmoid
              but zero-centered, which can speed up learning.
            </td>
          </tr>
          <tr>
            <td><strong>Softmax</strong></td>
            <td>f(xᵢ) = eˣⁱ / Σ(eˣʲ)</td>
            <td>(0, 1), sum=1</td>
            <td>
              Output layer for multi-class classification. Converts raw scores
              into probability distribution across classes.
            </td>
          </tr>
          <tr>
            <td><strong>Linear</strong></td>
            <td>f(x) = x</td>
            <td>(-∞, ∞)</td>
            <td>
              Output layer for regression tasks predicting continuous values
              (e.g., house prices, temperature).
            </td>
          </tr>
        </table>

        <div class="highlight-box" style="margin-top: 30px">
          <h4>Insights from Neural Network Playground Experiments:</h4>
          <p>
            I tested different activation functions on the same dataset and
            observed significant differences in learning speed and final
            accuracy. ReLU consistently trained faster and achieved better
            results on complex patterns like spirals. Tanh performed well on
            simpler datasets but struggled with very deep networks. Sigmoid
            worked best for binary classification but showed slow convergence.
            The Linear activation failed completely on non-linear patterns like
            XOR, demonstrating why non-linearity is essential. This hands-on
            experimentation reinforced that activation function choice
            significantly impacts model performance.
          </p>
        </div>
      </div>

      <!-- Slide 8: Component 5 - Loss Functions -->
      <div class="slide">
        <h2>Component 5: Loss Functions</h2>

        <div class="component-card">
          <div class="component-title">
            <div class="component-icon">L</div>
            What are Loss Functions?
          </div>
          <div class="component-description">
            Loss functions (also called cost functions or objective functions)
            measure how wrong a neural network's predictions are compared to
            actual values. They provide a single numerical score representing
            model performance. During training, the network's goal is to
            minimize this loss by adjusting weights. The choice of loss function
            depends on the type of problem being solved.
          </div>
        </div>

        <h3 style="margin-top: 30px">Common Loss Functions</h3>

        <div class="component-grid">
          <div class="component-card">
            <h4 style="color: #667eea">Mean Squared Error (MSE)</h4>
            <p style="margin: 10px 0">
              <strong>Formula:</strong> MSE = (1/n) × Σ(yᵢ - ŷᵢ)²
            </p>
            <p>
              <strong>Use:</strong> Regression problems (predicting continuous
              values)<br />
              <strong>How it works:</strong> Calculates the average squared
              difference between predicted and actual values. Larger errors are
              penalized more heavily due to squaring. This encourages the
              network to avoid large mistakes.
            </p>
          </div>

          <div class="component-card">
            <h4 style="color: #667eea">Binary Cross-Entropy</h4>
            <p style="margin: 10px 0">
              <strong>Formula:</strong> BCE = -[y×log(ŷ) + (1-y)×log(1-ŷ)]
            </p>
            <p>
              <strong>Use:</strong> Binary classification (two classes)<br />
              <strong>How it works:</strong> Measures how far predicted
              probabilities are from actual binary labels (0 or 1). Penalizes
              confident wrong predictions more heavily than uncertain ones.
            </p>
          </div>

          <div class="component-card">
            <h4 style="color: #667eea">Categorical Cross-Entropy</h4>
            <p style="margin: 10px 0">
              <strong>Formula:</strong> CCE = -Σ(yᵢ × log(ŷᵢ))
            </p>
            <p>
              <strong>Use:</strong> Multi-class classification (more than two
              classes)<br />
              <strong>How it works:</strong> Extends binary cross-entropy to
              multiple classes. Compares predicted probability distribution
              against true class distribution.
            </p>
          </div>
        </div>

        <h3 style="margin-top: 30px">Why Loss Functions Matter</h3>
        <div class="key-points">
          <ul>
            <li>
              <strong>Training Guidance:</strong> Loss functions provide the
              signal for how to adjust weights. Without them, the network has no
              way to know if it's improving.
            </li>
            <li>
              <strong>Optimization Target:</strong> The entire training process
              aims to find weights that minimize the loss function value.
            </li>
            <li>
              <strong>Performance Monitoring:</strong> Tracking loss over
              training iterations shows whether the model is learning.
              Decreasing loss indicates improvement.
            </li>
            <li>
              <strong>Problem-Specific:</strong> Different tasks require
              different loss functions. Using MSE for classification or
              cross-entropy for regression would produce poor results.
            </li>
          </ul>
        </div>

        <div class="highlight-box">
          <h4>Neural Network Playground Observation:</h4>
          <p>
            The playground displays training and test loss in real-time. I
            observed that on datasets with noise, the training loss continued
            decreasing while test loss eventually plateaued or increased,
            indicating overfitting. This visual feedback helped me understand
            that minimizing training loss alone isn't the goal—we need to
            balance it with generalization to unseen data.
          </p>
        </div>
      </div>

      <!-- Slide 9: Component 6 - Optimization Algorithms -->
      <div class="slide">
        <h2>Component 6: Optimization Algorithms</h2>

        <div class="component-card">
          <div class="component-title">
            <div class="component-icon">⚙</div>
            What are Optimization Algorithms?
          </div>
          <div class="component-description">
            Optimization algorithms (also called optimizers) are the methods
            used to adjust neural network weights during training to minimize
            the loss function. They determine how the network learns from its
            mistakes. The optimizer takes the gradient (direction and magnitude
            of steepest loss increase) and uses it to update weights in the
            opposite direction, gradually moving toward better predictions.
          </div>
        </div>

        <h3 style="margin-top: 30px">Popular Optimization Algorithms</h3>

        <table class="comparison-table">
          <tr>
            <th>Algorithm</th>
            <th>How It Works</th>
            <th>Advantages</th>
            <th>Disadvantages</th>
          </tr>
          <tr>
            <td><strong>SGD</strong><br />(Stochastic Gradient Descent)</td>
            <td>
              Updates weights using gradient calculated from each training
              example or small batch. Simple and foundational approach.
            </td>
            <td>
              Simple, works well with proper tuning, low memory requirements
            </td>
            <td>
              Slow convergence, sensitive to learning rate, can get stuck in
              local minima
            </td>
          </tr>
          <tr>
            <td><strong>Adam</strong><br />(Adaptive Moment Estimation)</td>
            <td>
              Combines momentum and adaptive learning rates. Maintains moving
              averages of gradients and their squares for each parameter.
            </td>
            <td>
              Fast convergence, works well with default parameters, handles
              sparse gradients well
            </td>
            <td>Can overfit on some problems, requires more memory</td>
          </tr>
          <tr>
            <td><strong>RMSprop</strong></td>
            <td>
              Adapts learning rate for each parameter based on recent gradient
              magnitudes. Good for handling different scales of features.
            </td>
            <td>
              Works well on non-stationary problems, good for recurrent neural
              networks
            </td>
            <td>
              Still requires learning rate tuning, can be slow on some problems
            </td>
          </tr>
          <tr>
            <td><strong>Momentum</strong></td>
            <td>
              Adds a fraction of previous update to current update, helping
              accelerate in relevant directions and dampen oscillations.
            </td>
            <td>
              Faster convergence than plain SGD, helps escape shallow local
              minima
            </td>
            <td>
              Introduces additional hyperparameter (momentum coefficient), can
              overshoot minima
            </td>
          </tr>
        </table>

        <h3 style="margin-top: 30px">Key Hyperparameters</h3>

        <div class="component-grid">
          <div class="component-card">
            <h4 style="color: #667eea">Learning Rate</h4>
            <p>
              Controls the step size of weight updates. Too high, and the
              network bounces around without converging. Too low, and training
              takes forever or gets stuck. Typical values: 0.001 to 0.1. This is
              often the most important hyperparameter to tune.
            </p>
          </div>

          <div class="component-card">
            <h4 style="color: #667eea">Batch Size</h4>
            <p>
              Number of training examples processed before updating weights.
              Smaller batches (32-128) provide noisy but frequent updates.
              Larger batches (256-1024) give stable gradients but slower
              updates. Choice affects both training speed and final performance.
            </p>
          </div>

          <div class="component-card">
            <h4 style="color: #667eea">Epochs</h4>
            <p>
              Number of times the entire training dataset is passed through the
              network. More epochs allow more learning but risk overfitting.
              Typical range: 10-100+ epochs depending on dataset size and
              complexity.
            </p>
          </div>
        </div>

        <div class="highlight-box" style="margin-top: 30px">
          <h4>Practical Insights from Neural Network Playground:</h4>
          <p>
            I experimented extensively with learning rates and observed dramatic
            differences. With learning rate 0.001, the spiral dataset trained
            slowly but steadily. At 0.1, the loss oscillated wildly and never
            converged properly. At 0.01, I found the sweet spot where training
            was both fast and stable. I also noticed that Adam optimizer
            consistently outperformed SGD on complex patterns, converging in
            fewer epochs. For noisy datasets, slower learning rates with more
            epochs produced better generalization than aggressive learning
            rates.
          </p>
        </div>
      </div>

      <!-- Slide 10: Putting It All Together -->
      <div class="slide">
        <h2>How Components Work Together</h2>

        <div class="architecture-diagram">
          <h3 style="color: #667eea; margin-bottom: 20px">
            The Complete Training Cycle
          </h3>

          <div style="text-align: left; max-width: 900px; margin: 0 auto">
            <div
              style="
                background: white;
                padding: 20px;
                border-radius: 10px;
                margin: 15px 0;
                border-left: 5px solid #667eea;
              "
            >
              <h4 style="color: #667eea; margin-bottom: 10px">
                1. Forward Propagation
              </h4>
              <p style="line-height: 1.8">
                Input data flows through layers. Each neuron computes a weighted
                sum of its inputs, adds bias, and applies an activation
                function. This process repeats layer by layer until reaching the
                output layer, which produces predictions.
              </p>
            </div>

            <div
              style="
                background: white;
                padding: 20px;
                border-radius: 10px;
                margin: 15px 0;
                border-left: 5px solid #764ba2;
              "
            >
              <h4 style="color: #764ba2; margin-bottom: 10px">
                2. Loss Calculation
              </h4>
              <p style="line-height: 1.8">
                The loss function compares predictions to actual values,
                producing a single number representing how wrong the network is.
                This provides the feedback signal needed for learning.
              </p>
            </div>

            <div
              style="
                background: white;
                padding: 20px;
                border-radius: 10px;
                margin: 15px 0;
                border-left: 5px solid #667eea;
              "
            >
              <h4 style="color: #667eea; margin-bottom: 10px">
                3. Backward Propagation
              </h4>
              <p style="line-height: 1.8">
                Gradients (derivatives of loss with respect to each weight) are
                calculated using the chain rule, flowing backward from output to
                input layer. This determines how much each weight contributed to
                the error.
              </p>
            </div>

            <div
              style="
                background: white;
                padding: 20px;
                border-radius: 10px;
                margin: 15px 0;
                border-left: 5px solid #764ba2;
              "
            >
              <h4 style="color: #764ba2; margin-bottom: 10px">
                4. Weight Update
              </h4>
              <p style="line-height: 1.8">
                The optimization algorithm uses gradients to update weights in
                the direction that reduces loss. The learning rate controls how
                large these updates are. This cycle repeats thousands of times
                until the network learns optimal weights.
              </p>
            </div>
          </div>
        </div>

        <div class="highlight-box" style="margin-top: 30px">
          <h4>Analogy: Learning to Throw a Basketball</h4>
          <p style="line-height: 1.8">
            Training a neural network is like learning to shoot a basketball.
            <strong>Layers</strong> are like the stages of your throw (stance,
            aim, release, follow-through). <strong>Neurons</strong> are
            individual muscle movements. <strong>Weights</strong> represent how
            much force each muscle applies.
            <strong>Activation functions</strong> decide when muscles activate.
            The <strong>loss function</strong> measures how far your shot was
            from the basket. The <strong>optimizer</strong> is your brain
            adjusting muscle memory after each attempt. With practice
            (training), you automatically adjust your form (weights) to
            consistently make baskets (minimize loss).
          </p>
        </div>
      </div>

      <!-- Slide 11: Summary and Key Insights -->
      <div class="slide">
        <h2>Summary: Key Insights and Importance of Visualization</h2>

        <h3 style="color: #667eea; margin-bottom: 20px">
          Why Visualizing Neural Networks Matters
        </h3>

        <p style="font-size: 1.1em; line-height: 1.8; margin-bottom: 30px">
          Visualizing neural networks transforms abstract mathematical concepts
          into intuitive, understandable representations. Through the Neural
          Network Playground experiments and creating this visual guide, I
          gained profound insights that wouldn't have been possible through
          theory alone. Visualization bridges the gap between equations and
          understanding, making neural networks accessible and debuggable.
        </p>

        <div class="summary-grid">
          <div class="insight-card">
            <h4>Intuition Building</h4>
            <p>
              Seeing how neurons activate, weights strengthen, and decision
              boundaries form provides intuition about what the network is
              learning. This visual feedback helps identify when models are
              underfitting, overfitting, or just right.
            </p>
          </div>

          <div class="insight-card">
            <h4>Debugging and Optimization</h4>
            <p>
              Visual tools reveal problems like vanishing gradients, dead
              neurons, or poor weight initialization. Watching loss curves helps
              detect overfitting early. Observing activation patterns identifies
              layers that aren't learning effectively.
            </p>
          </div>

          <div class="insight-card">
            <h4>Hyperparameter Understanding</h4>
            <p>
              Experimenting with different architectures, learning rates, and
              activation functions while seeing immediate visual feedback
              accelerated my understanding of how these choices impact
              performance. Theory explains what they do; visualization shows how
              they work.
            </p>
          </div>

          <div class="insight-card">
            <h4>Communication Tool</h4>
            <p>
              Visual representations enable effective communication with
              non-technical stakeholders. Instead of explaining backpropagation
              mathematically, showing data flow diagrams makes neural networks
              approachable to business leaders and clients.
            </p>
          </div>
        </div>

        <h3 style="color: #667eea; margin: 40px 0 20px 0">
          Key Takeaways from This Exercise
        </h3>

        <div class="key-points">
          <ul>
            <li>
              <strong>Architecture Matters:</strong> The number of layers and
              neurons per layer significantly impacts learning capacity. More
              complex datasets require deeper networks, but there's a point of
              diminishing returns where additional complexity hurts more than
              helps.
            </li>

            <li>
              <strong>Non-linearity is Essential:</strong> Activation functions
              enable neural networks to learn complex patterns. Without them,
              even the deepest network reduces to simple linear regression,
              incapable of solving problems like XOR or image classification.
            </li>

            <li>
              <strong>Learning Rate is Critical:</strong> This single
              hyperparameter can make the difference between successful training
              and complete failure. Too high causes instability; too low causes
              endless training. Finding the right value requires
              experimentation.
            </li>

            <li>
              <strong>Overfitting vs. Generalization:</strong> Networks can
              memorize training data while performing poorly on new data.
              Monitoring both training and test loss reveals this issue.
              Techniques like regularization and early stopping help maintain
              balance.
            </li>

            <li>
              <strong>Data Quality Impacts Everything:</strong> Neural networks
              are only as good as their training data. Noise, imbalanced
              classes, and insufficient examples all limit what networks can
              learn, regardless of architecture sophistication.
            </li>

            <li>
              <strong>Experimentation is Key:</strong> There's no universal
              formula for network design. Different problems require different
              architectures, and finding optimal configurations involves
              systematic experimentation and iterative refinement.
            </li>
          </ul>
        </div>

        <div class="highlight-box" style="margin-top: 30px">
          <h4>Personal Reflection on Learning:</h4>
          <p style="line-height: 1.8">
            Creating this visual guide while experimenting with the Neural
            Network Playground provided a dual learning experience. The hands-on
            experimentation made theoretical concepts tangible, while
            articulating my understanding through visualization deepened my
            comprehension. I moved from knowing definitions to truly
            understanding how components interact to enable learning. This
            exercise reinforced that machine learning isn't just
            mathematics—it's an iterative process of hypothesis,
            experimentation, observation, and refinement. The ability to
            visualize what networks are learning will be invaluable as I develop
            more complex AI systems throughout my career.
          </p>
        </div>
      </div>

      <!-- Slide 12: Applications and Future Directions -->
      <div class="slide">
        <h2>Real-World Applications</h2>

        <p style="font-size: 1.1em; line-height: 1.8; margin-bottom: 30px">
          Understanding neural network components enables building solutions for
          diverse real-world problems:
        </p>

        <div class="component-grid">
          <div class="component-card">
            <h4 style="color: #667eea">Computer Vision</h4>
            <p>
              Image classification, object detection, facial recognition,
              medical image analysis. CNNs with multiple convolutional layers
              automatically learn visual features from raw pixels.
            </p>
          </div>

          <div class="component-card">
            <h4 style="color: #667eea">Natural Language Processing</h4>
            <p>
              Language translation, sentiment analysis, chatbots, text
              generation. Recurrent networks and transformers process sequential
              text data to understand context and meaning.
            </p>
          </div>

          <div class="component-card">
            <h4 style="color: #667eea">Predictive Analytics</h4>
            <p>
              Stock price prediction, demand forecasting, credit risk
              assessment. Feed-forward networks learn complex non-linear
              relationships in tabular data that traditional methods miss.
            </p>
          </div>

          <div class="component-card">
            <h4 style="color: #667eea">Recommendation Systems</h4>
            <p>
              Product recommendations, content personalization, targeted
              advertising. Neural networks learn user preferences from behavior
              patterns and predict future interests.
            </p>
          </div>

          <div class="component-card">
            <h4 style="color: #667eea">Autonomous Systems</h4>
            <p>
              Self-driving cars, robotics, game-playing AI. Deep reinforcement
              learning combines neural networks with decision-making to learn
              optimal actions through trial and error.
            </p>
          </div>

          <div class="component-card">
            <h4 style="color: #667eea">Healthcare</h4>
            <p>
              Disease diagnosis, drug discovery, treatment personalization,
              patient outcome prediction. Networks analyze medical images,
              genetic data, and patient histories to assist healthcare
              providers.
            </p>
          </div>
        </div>

        <div class="highlight-box" style="margin-top: 40px">
          <h4>Connection to My Career Goals:</h4>
          <p style="line-height: 1.8">
            My goal is to develop intelligent systems that solve complex
            real-world problems. Understanding neural network fundamentals—from
            neuron computations to optimization algorithms—provides the
            foundation for building such systems. During my time at Accenture, I
            worked on full-stack applications, but my passion lies in creating
            AI-powered solutions. This deep dive into neural network
            architecture prepares me to design models that are not only accurate
            but also efficient, interpretable, and ethically developed. As I
            continue my master's program, I'll apply these principles to
            projects in NLP, computer vision, and predictive analytics, always
            keeping in mind that effective AI development requires both
            technical expertise and thoughtful design informed by visualization
            and experimentation.
          </p>
        </div>
      </div>
    </div>

    <footer>
      <p>
        <strong>Portfolio Artifact:</strong> Neural Network Architecture Visual
        Guide
      </p>
      <p>AIML-501 Model Development | Indiana Wesleyan University</p>
      <p>
        Muqeeth Mohammad | Master of Science in Computer Information Systems (AI
        Specialization)
      </p>
    </footer>
  </body>
</html>
